{
    "log_path": "/app/logs/itemspider/steamspider/2025-07-21T06_49_58.log",
    "json_path": "/app/logs/itemspider/steamspider/2025-07-21T06_49_58.json",
    "json_url": "http://127.0.0.1:6800/logs/itemspider/steamspider/2025-07-21T06_49_58.json",
    "size": 7239,
    "position": 7239,
    "status": "ok",
    "_head": 100,
    "head": "2025-07-21 06:50:04 [scrapy.utils.log] INFO: Scrapy 2.13.2 started (bot: itemspider)\n2025-07-21 06:50:04 [scrapy.utils.log] INFO: Versions:\n{'lxml': '5.4.0',\n 'libxml2': '2.13.8',\n 'cssselect': '1.3.0',\n 'parsel': '1.10.0',\n 'w3lib': '2.0.0',\n 'Twisted': '25.5.0',\n 'Python': '3.12.11 (main, Jul  1 2025, 02:44:10) [GCC 12.2.0]',\n 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',\n 'cryptography': '45.0.4',\n 'Platform': 'Linux-6.15.3-1-MANJARO-x86_64-with-glibc2.36'}\n2025-07-21 06:50:04 [scrapy.addons] INFO: Enabled addons:\n[]\n2025-07-21 06:50:04 [asyncio] DEBUG: Using selector: EpollSelector\n2025-07-21 06:50:04 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n2025-07-21 06:50:04 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n2025-07-21 06:50:04 [scrapy.extensions.telnet] INFO: Telnet Password: e762e5492eba7b6a\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.throttle.AutoThrottle']\n2025-07-21 06:50:04 [scrapy.crawler] INFO: Overridden settings:\n{'AUTOTHROTTLE_ENABLED': True,\n 'AUTOTHROTTLE_START_DELAY': 10,\n 'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.5,\n 'BOT_NAME': 'itemspider',\n 'CONCURRENT_REQUESTS': 1,\n 'DOWNLOAD_DELAY': 10,\n 'FEED_EXPORT_ENCODING': 'utf-8',\n 'HTTPCACHE_IGNORE_HTTP_CODES': [429],\n 'LOG_FILE': '/app/logs/itemspider/steamspider/2025-07-21T06_49_58.log',\n 'NEWSPIDER_MODULE': 'itemspider.spiders',\n 'SPIDER_MODULES': ['itemspider.spiders']}\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'itemspider.middlewares.ItemspiderDownloaderMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'itemspider.middlewares.ItemspiderSpiderMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2025-07-21 06:50:04 [py.warnings] WARNING: /usr/local/lib/python3.12/site-packages/scrapy/core/spidermw.py:98: ScrapyDeprecationWarning: The following enabled spider middlewares, directly or through their parent classes, define the deprecated process_start_requests() method: itemspider.middlewares.ItemspiderSpiderMiddleware. process_start_requests() has been deprecated in favor of a new method, process_start(), to support asynchronous code execution. process_start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace process_start_requests() with process_start(); note that process_start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when defining process_start_requests() in a spider middleware class, define process_start() as well. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n  warn(\n\n2025-07-21 06:50:04 [scrapy.core.spidermw] WARNING: Middleware itemspider.middlewares.ItemspiderSpiderMiddleware doesn't support asynchronous spider output, this is deprecated and will stop working in a future version of Scrapy. The middleware should be updated to support it. Please see https://docs.scrapy.org/en/latest/topics/coroutines.html#for-middleware-users for more information.\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled item pipelines:\n['itemspider.pipelines.ItemspiderPipeline',\n 'itemspider.pipelines.SaveToPostgreSQLPipeLine']\n2025-07-21 06:50:04 [scrapy.core.engine] INFO: Spider opened\n2025-07-21 06:50:04 [py.warnings] WARNING: /usr/local/lib/python3.12/site-packages/scrapy/core/spidermw.py:433: ScrapyDeprecationWarning: itemspider.spiders.steamspider.SteamspiderSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n  warn(\n\n2025-07-21 06:50:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2025-07-21 06:50:04 [steamspider] INFO: Spider opened: steamspider\n2025-07-21 06:50:04 [steamspider] INFO: Spider opened: steamspider\n2025-07-21 06:50:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n2025-07-21 06:50:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n2025-07-21 06:50:04 [scrapy.core.engine] INFO: Closing spider (shutdown)\n2025-07-21 06:50:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://steamcommunity.com/market/search/render/?query=&start=4000&count=100&search_descriptions=0&sort_column=price&sort_dir=desc&appid=730> (referer: None)\n2025-07-21 06:50:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 257,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 3477,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'elapsed_time_seconds': 2.222787,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2025, 7, 21, 6, 50, 6, 461275, tzinfo=datetime.timezone.utc),\n 'httpcompression/response_bytes': 25887,\n 'httpcompression/response_count': 1,\n 'items_per_minute': 0.0,\n 'log_count/DEBUG': 4,\n 'log_count/INFO': 13,\n 'log_count/WARNING': 3,\n 'memusage/max': 86396928,\n 'memusage/startup': 86396928,\n 'request_depth_max': 1,\n 'response_received_count': 1,\n 'responses_per_minute': 30.0,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 21,\n 'scheduler/enqueued/memory': 21,\n 'start_time': datetime.datetime(2025, 7, 21, 6, 50, 4, 238488, tzinfo=datetime.timezone.utc)}\n2025-07-21 06:50:06 [scrapy.core.engine] INFO: Spider closed (shutdown)",
    "tail": "2025-07-21 06:50:04 [scrapy.utils.log] INFO: Scrapy 2.13.2 started (bot: itemspider)\n2025-07-21 06:50:04 [scrapy.utils.log] INFO: Versions:\n{'lxml': '5.4.0',\n 'libxml2': '2.13.8',\n 'cssselect': '1.3.0',\n 'parsel': '1.10.0',\n 'w3lib': '2.0.0',\n 'Twisted': '25.5.0',\n 'Python': '3.12.11 (main, Jul  1 2025, 02:44:10) [GCC 12.2.0]',\n 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',\n 'cryptography': '45.0.4',\n 'Platform': 'Linux-6.15.3-1-MANJARO-x86_64-with-glibc2.36'}\n2025-07-21 06:50:04 [scrapy.addons] INFO: Enabled addons:\n[]\n2025-07-21 06:50:04 [asyncio] DEBUG: Using selector: EpollSelector\n2025-07-21 06:50:04 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n2025-07-21 06:50:04 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n2025-07-21 06:50:04 [scrapy.extensions.telnet] INFO: Telnet Password: e762e5492eba7b6a\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.throttle.AutoThrottle']\n2025-07-21 06:50:04 [scrapy.crawler] INFO: Overridden settings:\n{'AUTOTHROTTLE_ENABLED': True,\n 'AUTOTHROTTLE_START_DELAY': 10,\n 'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.5,\n 'BOT_NAME': 'itemspider',\n 'CONCURRENT_REQUESTS': 1,\n 'DOWNLOAD_DELAY': 10,\n 'FEED_EXPORT_ENCODING': 'utf-8',\n 'HTTPCACHE_IGNORE_HTTP_CODES': [429],\n 'LOG_FILE': '/app/logs/itemspider/steamspider/2025-07-21T06_49_58.log',\n 'NEWSPIDER_MODULE': 'itemspider.spiders',\n 'SPIDER_MODULES': ['itemspider.spiders']}\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'itemspider.middlewares.ItemspiderDownloaderMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'itemspider.middlewares.ItemspiderSpiderMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2025-07-21 06:50:04 [py.warnings] WARNING: /usr/local/lib/python3.12/site-packages/scrapy/core/spidermw.py:98: ScrapyDeprecationWarning: The following enabled spider middlewares, directly or through their parent classes, define the deprecated process_start_requests() method: itemspider.middlewares.ItemspiderSpiderMiddleware. process_start_requests() has been deprecated in favor of a new method, process_start(), to support asynchronous code execution. process_start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace process_start_requests() with process_start(); note that process_start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when defining process_start_requests() in a spider middleware class, define process_start() as well. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n  warn(\n\n2025-07-21 06:50:04 [scrapy.core.spidermw] WARNING: Middleware itemspider.middlewares.ItemspiderSpiderMiddleware doesn't support asynchronous spider output, this is deprecated and will stop working in a future version of Scrapy. The middleware should be updated to support it. Please see https://docs.scrapy.org/en/latest/topics/coroutines.html#for-middleware-users for more information.\n2025-07-21 06:50:04 [scrapy.middleware] INFO: Enabled item pipelines:\n['itemspider.pipelines.ItemspiderPipeline',\n 'itemspider.pipelines.SaveToPostgreSQLPipeLine']\n2025-07-21 06:50:04 [scrapy.core.engine] INFO: Spider opened\n2025-07-21 06:50:04 [py.warnings] WARNING: /usr/local/lib/python3.12/site-packages/scrapy/core/spidermw.py:433: ScrapyDeprecationWarning: itemspider.spiders.steamspider.SteamspiderSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n  warn(\n\n2025-07-21 06:50:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2025-07-21 06:50:04 [steamspider] INFO: Spider opened: steamspider\n2025-07-21 06:50:04 [steamspider] INFO: Spider opened: steamspider\n2025-07-21 06:50:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n2025-07-21 06:50:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n2025-07-21 06:50:04 [scrapy.core.engine] INFO: Closing spider (shutdown)\n2025-07-21 06:50:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://steamcommunity.com/market/search/render/?query=&start=4000&count=100&search_descriptions=0&sort_column=price&sort_dir=desc&appid=730> (referer: None)\n2025-07-21 06:50:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 257,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 3477,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'elapsed_time_seconds': 2.222787,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2025, 7, 21, 6, 50, 6, 461275, tzinfo=datetime.timezone.utc),\n 'httpcompression/response_bytes': 25887,\n 'httpcompression/response_count': 1,\n 'items_per_minute': 0.0,\n 'log_count/DEBUG': 4,\n 'log_count/INFO': 13,\n 'log_count/WARNING': 3,\n 'memusage/max': 86396928,\n 'memusage/startup': 86396928,\n 'request_depth_max': 1,\n 'response_received_count': 1,\n 'responses_per_minute': 30.0,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 21,\n 'scheduler/enqueued/memory': 21,\n 'start_time': datetime.datetime(2025, 7, 21, 6, 50, 4, 238488, tzinfo=datetime.timezone.utc)}\n2025-07-21 06:50:06 [scrapy.core.engine] INFO: Spider closed (shutdown)\n",
    "first_log_time": "2025-07-21 06:50:04",
    "latest_log_time": "2025-07-21 06:50:06",
    "runtime": "0:00:02",
    "first_log_timestamp": 1753080604,
    "latest_log_timestamp": 1753080606,
    "datas": [
        [
            "2025-07-21 06:50:04",
            0,
            0,
            0,
            0
        ]
    ],
    "pages": 1,
    "items": 0,
    "latest_matches": {
        "scrapy_version": "2.13.2",
        "telnet_console": "127.0.0.1:6024",
        "telnet_username": "",
        "telnet_password": "e762e5492eba7b6a",
        "resuming_crawl": "",
        "latest_offsite": "",
        "latest_duplicate": "",
        "latest_crawl": "2025-07-21 06:50:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://steamcommunity.com/market/search/render/?query=&start=4000&count=100&search_descriptions=0&sort_column=price&sort_dir=desc&appid=730> (referer: None)",
        "latest_stat": "2025-07-21 06:50:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)",
        "latest_scrape": "",
        "latest_item": ""
    },
    "latest_crawl_timestamp": 1753080606,
    "latest_scrape_timestamp": 0,
    "log_categories": {
        "critical_logs": {
            "count": 0,
            "details": []
        },
        "error_logs": {
            "count": 0,
            "details": []
        },
        "warning_logs": {
            "count": 3,
            "details": [
                "2025-07-21 06:50:04 [py.warnings] WARNING: /usr/local/lib/python3.12/site-packages/scrapy/core/spidermw.py:98: ScrapyDeprecationWarning: The following enabled spider middlewares, directly or through their parent classes, define the deprecated process_start_requests() method: itemspider.middlewares.ItemspiderSpiderMiddleware. process_start_requests() has been deprecated in favor of a new method, process_start(), to support asynchronous code execution. process_start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace process_start_requests() with process_start(); note that process_start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when defining process_start_requests() in a spider middleware class, define process_start() as well. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n  warn(\n",
                "2025-07-21 06:50:04 [scrapy.core.spidermw] WARNING: Middleware itemspider.middlewares.ItemspiderSpiderMiddleware doesn't support asynchronous spider output, this is deprecated and will stop working in a future version of Scrapy. The middleware should be updated to support it. Please see https://docs.scrapy.org/en/latest/topics/coroutines.html#for-middleware-users for more information.",
                "2025-07-21 06:50:04 [py.warnings] WARNING: /usr/local/lib/python3.12/site-packages/scrapy/core/spidermw.py:433: ScrapyDeprecationWarning: itemspider.spiders.steamspider.SteamspiderSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n  warn(\n"
            ]
        },
        "redirect_logs": {
            "count": 0,
            "details": []
        },
        "retry_logs": {
            "count": 0,
            "details": []
        },
        "ignore_logs": {
            "count": 0,
            "details": []
        }
    },
    "shutdown_reason": "Received SIGINT",
    "finish_reason": "shutdown",
    "crawler_stats": {
        "source": "log",
        "last_update_time": "2025-07-21 06:50:06",
        "last_update_timestamp": 1753080606,
        "downloader/request_bytes": 257,
        "downloader/request_count": 1,
        "downloader/request_method_count/GET": 1,
        "downloader/response_bytes": 3477,
        "downloader/response_count": 1,
        "downloader/response_status_count/200": 1,
        "elapsed_time_seconds": 2.222787,
        "finish_reason": "shutdown",
        "finish_time": "datetime.datetime(2025, 7, 21, 6, 50, 6, 461275, tzinfo=datetime.timezone.utc)",
        "httpcompression/response_bytes": 25887,
        "httpcompression/response_count": 1,
        "items_per_minute": 0.0,
        "log_count/DEBUG": 4,
        "log_count/INFO": 13,
        "log_count/WARNING": 3,
        "memusage/max": 86396928,
        "memusage/startup": 86396928,
        "request_depth_max": 1,
        "response_received_count": 1,
        "responses_per_minute": 30.0,
        "scheduler/dequeued": 1,
        "scheduler/dequeued/memory": 1,
        "scheduler/enqueued": 21,
        "scheduler/enqueued/memory": 21,
        "start_time": "datetime.datetime(2025, 7, 21, 6, 50, 4, 238488, tzinfo=datetime.timezone.utc)"
    },
    "last_update_time": "2025-07-21 06:50:12",
    "last_update_timestamp": 1753080612,
    "logparser_version": "0.8.4",
    "crawler_engine": {}
}